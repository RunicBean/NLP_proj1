{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_file = '../data/news_tokenized.csv'\n",
    "word_embedding_file = '../data/no_symbol.word2vec'\n",
    "weight_file = '../../main/data/total_vocab.txt'\n",
    "similarity_file = '../data/similar_evaluation.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tokenized_file - 预处理好的csv文件\n",
    "* word_embedding_file - word_embedding_train生成好的词向量文件\n",
    "* weight_file - word_embedding_train生成好的词频文件\n",
    "* similarity_file - 最后生成的相似度文件，只是有个view，不是最后的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(text):\n",
    "    new_text = ''\n",
    "    text_sentences = text.split('\\n')\n",
    "    for sentence in text_sentences:\n",
    "        sentence_phrases = re.findall(r'[\\w\\s]+', sentence)\n",
    "        new_text += ' '.join(sentence_phrases) + '\\n'\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordmap(textfile):\n",
    "    words={}\n",
    "    We = []\n",
    "    f = open(textfile,'r')\n",
    "    lines = f.readlines()\n",
    "    if len(lines[0].split()) < 5:\n",
    "        lines = lines[1:]\n",
    "    for (n,i) in enumerate(lines):\n",
    "        i=i.split()\n",
    "        j = 1\n",
    "        v = []\n",
    "        while j < len(i):\n",
    "            v.append(float(i[j]))\n",
    "            j += 1\n",
    "        words[i[0]]=n\n",
    "        We.append(v)\n",
    "    return (words, np.array(We))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*getWordmap*载入word embedding到内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordWeight(weightfile, a=1e-3):\n",
    "    if a <=0: # when the parameter makes no sense, use unweighted\n",
    "        a = 1.0\n",
    "\n",
    "    word2weight = {}\n",
    "    with open(weightfile) as f:\n",
    "        lines = f.readlines()\n",
    "    N = 0\n",
    "    for i in lines:\n",
    "        if \"appearance\" in i:\n",
    "            continue\n",
    "        i=i.strip()\n",
    "        if(len(i) > 0):\n",
    "            i=i.split()\n",
    "            if(len(i) == 2):\n",
    "                word2weight[i[0]] = float(i[1])\n",
    "                N += float(i[1])\n",
    "            else:\n",
    "                print(i)\n",
    "    for key, value in word2weight.items():\n",
    "        word2weight[key] = a / (a + value/N)\n",
    "    return word2weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*getWordWeight*利用词频计算每一个词的权重 $\\frac{a}{a+Pr(word)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeight(words, word2weight):\n",
    "    weight4ind = {}\n",
    "    for word, ind in words.items():\n",
    "        if word in word2weight:\n",
    "            weight4ind[ind] = word2weight[word]\n",
    "        else:\n",
    "            weight4ind[ind] = 1.0\n",
    "    return weight4ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_initialize(word_embedding_file, weight_file, weight_para):\n",
    "    (words2index, words_embedding) = getWordmap(word_embedding_file)\n",
    "    word2weight = getWordWeight(weight_file, weight_para)\n",
    "    weight4ind = getWeight(words2index, word2weight)\n",
    "    return weight4ind, words2index, words_embedding\n",
    "\n",
    "weight4ind, words2index, words_embedding = map_initialize(word_embedding_file, weight_file, weight_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookupIDX(words,w):\n",
    "    w = w.lower()\n",
    "    if len(w) > 1 and w[0] == '#':\n",
    "        w = w.replace(\"#\",\"\")\n",
    "    if w in words:\n",
    "        return words[w]\n",
    "    elif 'UUUNKKK' in words:\n",
    "        return words['UUUNKKK']\n",
    "    else:\n",
    "        return len(words) - 1\n",
    "\n",
    "def getSeq(p1,words):\n",
    "    p1 = p1.split()\n",
    "    X1 = []\n",
    "    for i in p1:\n",
    "        X1.append(lookupIDX(words,i))\n",
    "    return X1\n",
    "\n",
    "def prepare_data(list_of_seqs):\n",
    "    lengths = [len(s) for s in list_of_seqs]\n",
    "    n_samples = len(list_of_seqs)\n",
    "    maxlen = np.max(lengths)\n",
    "    x = np.zeros((n_samples, maxlen)).astype('int32')\n",
    "    x_mask = np.zeros((n_samples, maxlen)).astype('float32')\n",
    "    for idx, s in enumerate(list_of_seqs):\n",
    "        x[idx, :lengths[idx]] = s\n",
    "        x_mask[idx, :lengths[idx]] = 1.\n",
    "    x_mask = np.asarray(x_mask, dtype='float32')\n",
    "    return x, x_mask\n",
    "\n",
    "def sentences2idx(sentences, words):\n",
    "    \"\"\"\n",
    "    Given a list of sentences, output array of word indices that can be fed into the algorithms.\n",
    "    :param sentences: a list of sentences\n",
    "    :param words: a dictionary, words['str'] is the indices of the word 'str'\n",
    "    :return: x1, m1. x1[i, :] is the word indices in sentence i, m1[i,:] is the mask for sentence i (0 means no word at the location)\n",
    "    \"\"\"\n",
    "    seq1 = []\n",
    "    for i in sentences:\n",
    "        seq1.append(getSeq(i,words))\n",
    "    x1,m1 = prepare_data(seq1)\n",
    "    return x1, m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*sentences2inx*的作用是将每个句子中分好的词用words2index变量里的word对应的index占位，x1用index占位，m1用1来占位。  \n",
    "例如： “日 与 月” ：array[9, 8, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2weight(seq, mask, weight4ind):\n",
    "    weight = np.zeros(seq.shape).astype('float32')\n",
    "    for i in range(seq.shape[0]):\n",
    "        for j in range(seq.shape[1]):\n",
    "            if mask[i,j] > 0 and seq[i,j] >= 0:\n",
    "                weight[i,j] = weight4ind[seq[i,j]]\n",
    "    weight = np.asarray(weight, dtype='float32')\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*seq2weight*将上面index占好的位置用权重替换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pc(X,npc=1):\n",
    "    \"\"\"\n",
    "    Compute the principal components. DO NOT MAKE THE DATA ZERO MEAN!\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: component_[i,:] is the i-th pc\n",
    "    \"\"\"\n",
    "    svd = TruncatedSVD(n_components=npc, n_iter=7, random_state=0)\n",
    "    svd.fit(X)\n",
    "    return svd.components_\n",
    "\n",
    "def remove_pc(X, npc=1):\n",
    "    \"\"\"\n",
    "    Remove the projection on the principal components\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: XX[i, :] is the data point after removing its projection\n",
    "    \"\"\"\n",
    "    pc = compute_pc(X, npc)\n",
    "    if npc==1:\n",
    "        XX = X - X.dot(pc.transpose()) * pc\n",
    "    else:\n",
    "        XX = X - X.dot(pc.transpose()).dot(pc)\n",
    "    return XX\n",
    "\n",
    "def SIF_embedding(We, x, w, params):\n",
    "    \"\"\"\n",
    "    Compute the scores between pairs of sentences using weighted average + removing the projection on the first principal component\n",
    "    :param We: We[i,:] is the vector for word i\n",
    "    :param x: x[i, :] are the indices of the words in the i-th sentence\n",
    "    :param w: w[i, :] are the weights for the words in the i-th sentence\n",
    "    :param params.rmpc: if >0, remove the projections of the sentence embeddings to their first principal component\n",
    "    :return: emb, emb[i, :] is the embedding for sentence i\n",
    "    \"\"\"\n",
    "    emb = get_weighted_average(We, x, w)\n",
    "    if  params.rmpc > 0:\n",
    "        emb = remove_pc(emb, params.rmpc)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SIF_embedding*建立句子向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_embedding(sentences):\n",
    "\n",
    "    \"\"\"\n",
    "    return: embedding: ndarray, shape (n_samples, vector_space_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    sequence_matrix, mask_matrix = sentences2idx(sentences, words2index)\n",
    "    weight_matrix = seq2weight(sequence_matrix, mask_matrix, weight4ind)\n",
    "    params = sparams.params()\n",
    "    # 移除前多少个主成分\n",
    "    params.rmpc = rm_pc\n",
    "\n",
    "    embedding = SIF_embedding(words_embedding, sequence_matrix, weight_matrix, params)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_from_content(title, content, sentences):\n",
    "    title_embedding = get_sentences_embedding([title])\n",
    "    sentences_embedding = get_sentences_embedding(sentences)\n",
    "    contents_embedding = get_sentences_embedding([remove_symbols(content.replace('\\n', ' '))])\n",
    "\n",
    "    return title_embedding, sentences_embedding, contents_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_vector_distance(vector1, vector2):\n",
    "    inner_prod = (vector1 * vector2).sum()\n",
    "    vctr1_norm = np.sqrt((vector1 * vector1).sum())\n",
    "    vctr2_norm = np.sqrt((vector2 * vector2).sum())\n",
    "    cos_distance = inner_prod / (vctr1_norm * vctr2_norm)\n",
    "    return cos_distance\n",
    "\n",
    "\n",
    "def eval_vector_similar(vector1, vector2):\n",
    "    return (cal_vector_distance(vector1, vector2) + 1) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算vector cos距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_from_file():\n",
    "\n",
    "    news_token_df = pd.read_csv(tokenized_file)\n",
    "\n",
    "    for i in news_token_df.index:\n",
    "        sentences = [remove_symbols(sen) for sen in news_token_df.loc[i]['content'].split('\\n')]\n",
    "        title_embedding, sentences_embedding, contents_embedding = \\\n",
    "            get_vectors_from_content(news_token_df.loc[i]['title'],\n",
    "                                     news_token_df.loc[i]['content'],\n",
    "                                     sentences)\n",
    "        index = 0\n",
    "        for sentence in sentences_embedding:\n",
    "            # 计算句子与content、title相似度\n",
    "            content_sim = eval_vector_similar(sentence, contents_embedding[0])\n",
    "            title_sim = eval_vector_similar(sentence, title_embedding[0])\n",
    "            # 标题和内容如何加权？\n",
    "            total_sim = (content_sim + title_sim) / 2\n",
    "            save(news_token_df.loc[i]['doc_id'], sentences[index], content_sim, title_sim, total_sim)\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(doc_id, sentence, content_sim, title_sim, total_sim):\n",
    "    if not os.path.exists(similarity_file):\n",
    "        with open(similarity_file, 'w') as sewh:\n",
    "            sewh.write('doc_id,sentence,content_sim,title_sim,total_sim')\n",
    "    with open(similarity_file, 'a') as sewh:\n",
    "        sewh.write('{},{},{},{},{}\\n'.format(doc_id, sentence.strip(), content_sim, title_sim, total_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_vectors_from_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
